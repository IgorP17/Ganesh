Отлично! Давайте разберем ваш мини-проект по шагам. Я постараюсь максимально подробно описать каждый этап.
1. Установка Kubernetes на Linux Mint (VirtualBox)
Шаг 1: Установка Docker

Kubernetes использует Docker для управления контейнерами. Установим Docker:
bash
Copy

sudo apt update
sudo apt install -y docker.io
sudo systemctl enable docker
sudo systemctl start docker

Шаг 2: Установка kubectl, kubeadm и kubelet

Установим инструменты для работы с Kubernetes:
bash
Copy

sudo apt update
sudo apt install -y apt-transport-https ca-certificates curl
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/kubernetes-archive-keyring.gpg
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
-- тут будет ошибка ппц
    sudo rm /etc/apt/sources.list.d/kubernetes.list
    -- тут будет подстава
    echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

    sudo mkdir -p /etc/apt/keyrings
    curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
    sudo apt update

sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

Убедитесь, что установка прошла успешно:
kubectl version --client
kubeadm version

Шаг 3: Инициализация Kubernetes

Инициализируем кластер Kubernetes:
bash
Copy

sudo kubeadm init --pod-network-cidr=10.244.0.0/16
-- remote version is much newer: v1.32.2; falling back to: stable-1.28
    sudo apt remove kubeadm kubelet kubectl
    sudo apt update
    sudo apt install -y apt-transport-https ca-certificates curl
    sudo mkdir -p /etc/apt/keyrings
    curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
    echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

    sudo apt install -y kubelet kubeadm kubectl
    sudo apt-mark hold kubelet kubeadm kubectl

    kubeadm version
    kubectl version --client
    kubelet --version

    sudo kubeadm init --pod-network-cidr=10.244.0.0/16

    ошибка по swap
    sudo swapon --show
        sudo swapoff -a - до перезанрузки отключить свап
        sudo nano /etc/fstab
        комментаррим
        # /swapfile none swap sw 0 0

    все равно не ставимся:
        Шаг 1: Сбросьте состояние kubeadm

        Выполните команду для сброса состояния кластера:
        bash
        Copy

        sudo kubeadm reset

        Эта команда:

            Останавливает все запущенные компоненты Kubernetes.

            Удаляет конфигурационные файлы.

            Очищает сетевые настройки.

        Шаг 2: Удалите оставшиеся файлы

        Если после kubeadm reset остались файлы, удалите их вручную:
        bash
        Copy

        sudo rm -rf /etc/kubernetes/
        sudo rm -rf /var/lib/etcd/
        sudo rm -rf $HOME/.kube/

        Шаг 3: Убедитесь, что порты освобождены

        Проверьте, какие процессы используют порты 6443, 10259, 10257, 2379 и 2380:
        bash
        Copy

        sudo netstat -tuln | grep -E '6443|10259|10257|2379|2380'

        Если порты заняты, найдите и завершите процессы, которые их используют. Например:
        bash
        Copy

        sudo lsof -i :6443  # Найти процесс, использующий порт 6443
        sudo kill <PID>     # Завершить процесс по его ID

        2. Повторная инициализация кластера

        После очистки системы выполните инициализацию кластера снова:
        bash
        Copy

        sudo kubeadm init --pod-network-cidr=10.244.0.0/16

#############################################
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.129:6443 --token p53tmr.2wpze8p6f2xxbpj9 \
        --discovery-token-ca-cert-hash sha256:56dddac85a4322aa51d262778b738a2652687b6e7befcc96eb4f017499cb044d

##############################################



После завершения команды вы получите команду для подключения других узлов (если они есть) и команду для настройки kubectl:
bash
Copy

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

-----here-----
Шаг 4: Установка сетевого плагина

Установим сетевой плагин (например, Flannel):
bash
Copy

kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

Шаг 5: Проверка кластера

Убедимся, что кластер работает:
bash
Copy

kubectl get nodes

2. Установка Kafka и создание топика
Шаг 1: Установка Kafka

Установим Kafka с помощью Helm (пакетного менеджера для Kubernetes):
bash
Copy

# Установим Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Добавим репозиторий Bitnami
helm repo add bitnami https://charts.bitnami.com/bitnami

# Установим Kafka
helm install my-kafka bitnami/kafka

Шаг 2: Создание топика

Создадим топик в Kafka:
bash
Copy

kubectl exec -it my-kafka-0 -- kafka-topics.sh --create --topic my-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

3. Пример приложения на Java
Шаг 1: Создание проекта

Создадим проект с помощью Maven:
bash
Copy

mvn archetype:generate -DgroupId=com.example -DartifactId=kafka-app -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
cd kafka-app

Шаг 2: Добавление зависимостей

Добавим зависимости в pom.xml:
xml
Copy

<dependencies>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>3.1.0</version>
    </dependency>
    <dependency>
        <groupId>com.sparkjava</groupId>
        <artifactId>spark-core</artifactId>
        <version>2.9.4</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-simple</artifactId>
        <version>1.7.36</version>
    </dependency>
</dependencies>

Run HTML
Шаг 3: Написание кода

Создадим класс App.java:
java
Copy

package com.example;

import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import spark.Spark;

import java.util.Properties;

public class App {
    public static void main(String[] args) {
        // Настройка Kafka Producer
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "my-kafka:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        // Настройка Spark (веб-сервер)
        Spark.port(8080);
        Spark.post("/send", (req, res) -> {
            String message = req.body();
            System.out.println("Received message: " + message);

            // Отправка сообщения в Kafka
            ProducerRecord<String, String> record = new ProducerRecord<>("my-topic", message);
            producer.send(record, (metadata, exception) -> {
                if (exception != null) {
                    System.err.println("Error sending message: " + exception.getMessage());
                } else {
                    System.out.println("Message sent to topic: " + metadata.topic());
                }
            });

            return "Message sent to Kafka";
        });
    }
}

Шаг 4: Сборка проекта

Соберем проект:
bash
Copy

mvn clean package

4. Деплой приложения в Kubernetes
Шаг 1: Создание Dockerfile

Создадим Dockerfile:
Dockerfile
Copy

FROM openjdk:11-jre-slim
COPY target/kafka-app-1.0-SNAPSHOT.jar /app.jar
CMD ["java", "-jar", "/app.jar"]

Шаг 2: Сборка Docker-образа

Соберем Docker-образ:
bash
Copy

docker build -t my-kafka-app .

Шаг 3: Создание манифеста Kubernetes

Создадим файл deployment.yaml:
yaml
Copy

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-app
  template:
    metadata:
      labels:
        app: kafka-app
    spec:
      containers:
      - name: kafka-app
        image: my-kafka-app
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-app
spec:
  selector:
    app: kafka-app
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: NodePort

Шаг 4: Деплой приложения

Применим манифест:
bash
Copy

kubectl apply -f deployment.yaml

Шаг 5: Проверка логов

Проверим логи пода:
bash
Copy

kubectl get pods
kubectl logs <pod-name>

Шаг 6: Тестирование

Отправим POST-запрос:
bash
Copy

curl -X POST -d "Hello Kafka" http://<node-ip>:<node-port>/send

Итог

Теперь у вас есть:

    Кластер Kubernetes на Linux Mint.

    Kafka с топиком my-topic.

    Java-приложение, которое принимает POST-запросы и отправляет их в Kafka.

    Приложение развернуто в Kubernetes, и вы можете проверять логи.

Если что-то не работает или есть вопросы, пишите! 😊
New chat
